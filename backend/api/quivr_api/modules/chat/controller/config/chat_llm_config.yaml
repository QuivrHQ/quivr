workflow_config:
  name: "Chat LLM"
  nodes:
    - name: "START"
      edges: ["filter_history"]

    - name: "filter_history"
      edges: ["generate_chat_llm"]

    - name: "generate_chat_llm" # the name of the last node, from which we want to stream the answer to the user, should always start with "generate"
      edges: ["END"]
# Maximum number of previous conversation iterations
# to include in the context of the answer
max_history: 10

#prompt: "my prompt"

llm_config:
  max_input_tokens: 2000

  # Maximum number of tokens to pass to the LLM
  # as a context to generate the answer
  max_output_tokens: 2000

  temperature: 0.7
  streaming: true
